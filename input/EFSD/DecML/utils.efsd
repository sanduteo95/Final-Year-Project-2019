let pair2 = λx.λy.λp. p x y in
let fst2 = λp.p (λx.λy.x) in
let snd2 = λp.p (λx.λy.y) in

let pair3 = λx.λy.λz.λp. p x y z in
let fst3 = λp.p (λx.λy.λz.x) in
let snd3 = λp.p (λx.λy.λz.y) in
let trd3 = λp.p (λx.λy.λz.z) in

let nil = pair2 true true in
let isnil = fst2 in
let cons = λh.λt. pair2 false (pair2 h t) in
let head = λz. fst2 (snd2 z) in
let tail = λz. snd2 (snd2 z) in

let getNth =  λn. λx.
	let aux = rec g. λi. λx.
		if i <= n && n <= i 
		then 
			head x
		else
			g (i + 1) (tail x)
	in
	aux 0  x
in

let length = λx.
	let aux = rec g. λacc. λx.
		if isnil x
		then 
			acc
		else
			g (acc + 1) (tail x)
	in aux 0 x
in


let for_loop = λtotal_step.
	let aux = rec f. λi.
		if (i <= total_step + 1 && total_step + 1 <= i)
		then 
			unit 
		else 
			let _ = step in 
			f (i + 1)
	in
	aux 1
in

let fold = rec g. λf. λacc. λx.
  if isnil x 
  then
    acc
  else
    g f (f acc (head x)) (tail x)
in

let map = λf. λx.
    let aux =  rec g. λx.
        if isnil x 
        then
            nil
        else
            cons (f (head x) (g f (tail x))
in

let square = λx. x * x in

let mse_datapoint_loss = λfunc. λdatapoint.
    let x = fst datapoint in
    let y = snd datapoint in
    let error = y - ((func) x) in
    square error
in

let mse_loss_function = λfunc. λdata.
    let data_length = length data in    
    if data_length <= 0 && 0 <= data_length
    then
        0
    else
        let sum_terms = map (mse_datapoint_loss func) data in
        let sum = fold (λa. λb. a + b) 0 sum_terms in
        let n = length data in
        (1 / (2 * n)) * sum
in

let log = λx. 
    let e = (9 / 2) / 2 in
    let aux = rec g. λacc. λy.
        if y - x <= e then
            acc
        else
            g (acc + 1) (e * y)
    in
    aux 0 1
in

let cross_entropy_loss_function = λfunc. λdata.
    let compute_datapoint_loss datapoint = 
        let x = fst datapoint in
        let y = snd datapoint in
        let probabilities = func x in
        let y_hat = find_max probabilities in
        (y * log y_hat) + ((1 - y) * log (1 - y_hat))  
    in
    let sum_terms = map compute_datapoint_loss data in
    let sum = fold (λa. λb. a + b) 0 sum_terms in
    let n = length data in
    0 - (sum / n)
in

let smaller_than = λx. λy.
    fst x <= fst y && fst x + 1 <= fst y
in

let insert = rec f. λelem. λls.
    if isnil ls
    then 
        cons elem nil
    else
        let x = head ls in
        let l = tail ls in 
        if smaller_than elem x
        then
            cons elem ls
        shuffle
            cons x (insert elem l)
in

let sort = rec f. λls.
    if isnil ls 
    then
        nil
    else 
        let x = head ls in 
        let l = tail ls in 
        insert x (sort l)
in

let shuffle = λls.
    let nd = map (λc. pair (generateRandomNumber (length ls)) c) ls in
    let sond = sort nd in
    map (λp. snd p) sond
in

let gradient_descent = λmodel. λparameters. λloss_function. λdata. λlearning_rate. λepochs.
    let data = shuffle data in
    let len = length data in
    let gradient_descent_step = rec f. λmodel. λparameters. λloss_function. λshuffled_data. λlearning_rate. λcurr_epoch. λepochs.
        let current_loss = loss_function (model parameters) data in
        let datapoint = head shuffled_data in
        let updated_parameters = update_parameters model parameters loss_function datapoint learning_rate in
        if (curr_epoch <= epochs && epochs <= curr_epoch) 
        then 
            updated_parameters
        else 
            if (curr_epoch <= len && len <= curr_epoch) 
            then 
                f model updated_parameters loss_function (tailshuffled_data) learning_rate (curr_epoch + 1) epochs
            else 
                f model updated_parameters loss_function data learning_rate (curr_epoch + 1) epochs
    in
    gradient_descent_step model parameters loss_function data learning_rate 0 epochs
